- model: sim.quiz
  pk: 1
  fields:
    name: GCC PDE Quiz 01
- model: sim.question
  pk: 1
  fields:
    quiz: 1
    text: You work for a mid-sized enterprise that needs to move its operational system
      transaction data from an on-premises database to GCP. The database is about
      20TB in size. Which database should you choose?
- model: sim.answer
  pk: 1
  fields:
    question: 1
    text: Cloud SQL
    is_correct: true
- model: sim.answer
  pk: 2
  fields:
    question: 1
    text: Cloud Bigtable
    is_correct: false
- model: sim.answer
  pk: 3
  fields:
    question: 1
    text: Cloud Spanner
    is_correct: false
- model: sim.answer
  pk: 4
  fields:
    question: 1
    text: Cloud Datastore
    is_correct: false
- model: sim.question
  pk: 2
  fields:
    quiz: 1
    text: You need to choose a database for a new project that has the following requirements-
      Fully managed, able to automatically scale up, transactionally consistent, able
      to scale up to 6 TB, and able to be queried using SQL. Which database do you
      choose?
- model: sim.answer
  pk: 5
  fields:
    question: 2
    text: Cloud SQL
    is_correct: true
- model: sim.answer
  pk: 6
  fields:
    question: 2
    text: Cloud Bigtable
    is_correct: false
- model: sim.answer
  pk: 7
  fields:
    question: 2
    text: Cloud Spanner
    is_correct: false
- model: sim.answer
  pk: 8
  fields:
    question: 2
    text: Cloud Datastore
    is_correct: false
- model: sim.question
  pk: 3
  fields:
    quiz: 1
    text: You need to deploy additional dependencies to all nodes of a Cloud Dataproc
      cluster at startup using an existing initialization action. Company security
      policies require that Cloud Dataproc nodes do not have access to the Internet
      so public initialization actions cannot fetch resources. What should you do?
- model: sim.answer
  pk: 9
  fields:
    question: 3
    text: Deploy the Cloud SQL Proxy on the Cloud Dataproc master
    is_correct: false
- model: sim.answer
  pk: 10
  fields:
    question: 3
    text: Use an SSH tunnel to give the Cloud Dataproc cluster access to the Internet
    is_correct: false
- model: sim.answer
  pk: 11
  fields:
    question: 3
    text: Copy all dependencies to a Cloud Storage bucket within your VPC security
      perimeter
    is_correct: true
- model: sim.answer
  pk: 12
  fields:
    question: 3
    text: Use Resource Manager to add the service account used by the Cloud Dataproc
      cluster to the Network User role
    is_correct: false
- model: sim.question
  pk: 4
  fields:
    quiz: 1
    text: You are planning to migrate your current on-premises Apache Hadoop deployment
      to the cloud. You need to ensure that the deployment is as fault-tolerant and
      cost-effective as possible for long-running batch jobs. You want to use a managed
      service. What should you do?
- model: sim.answer
  pk: 13
  fields:
    question: 4
    text: Deploy a Dataproc cluster. Use a standard persistent disk and 50% preemptible
      workers. Store data in Cloud Storage, and change references in scripts from
      hdfs:// to gs://
    is_correct: true
- model: sim.answer
  pk: 14
  fields:
    question: 4
    text: Deploy a Dataproc cluster. Use an SSD persistent disk and 50% preemptible
      workers. Store data in Cloud Storage, and change references in scripts from
      hdfs:// to gs://
    is_correct: false
- model: sim.answer
  pk: 15
  fields:
    question: 4
    text: Install Hadoop and Spark on a 10-node Compute Engine instance group with
      standard instances. Install the Cloud Storage connector, and store the data
      in Cloud Storage. Change references in scripts from hdfs:// to gs://
    is_correct: false
- model: sim.answer
  pk: 16
  fields:
    question: 4
    text: Install Hadoop and Spark on a 10-node Compute Engine instance group with
      preemptible instances. Store data in HDFS. Change references in scripts from
      hdfs:// to gs://
    is_correct: false
- model: sim.question
  pk: 5
  fields:
    quiz: 1
    text: Your company is selecting a system to centralize data ingestion and delivery.
      You are considering messaging and data integration systems to address the requirements.
      The key requirements are- The ability to seek to a particular offset in a topic,
      support for publish/subscribe semantics on hundreds of topics, and retain per-key
      ordering. Which system should you choose?
- model: sim.answer
  pk: 17
  fields:
    question: 5
    text: Apache Kafka
    is_correct: true
- model: sim.answer
  pk: 18
  fields:
    question: 5
    text: Cloud Storage
    is_correct: false
- model: sim.answer
  pk: 19
  fields:
    question: 5
    text: Dataflow
    is_correct: false
- model: sim.answer
  pk: 20
  fields:
    question: 5
    text: Firebase Cloud Messaging
    is_correct: false
- model: sim.question
  pk: 6
  fields:
    quiz: 1
    text: You plan to deploy Cloud SQL using MySQL. You need to ensure high availability
      in the event of a zone failure. What should you do?
- model: sim.answer
  pk: 21
  fields:
    question: 6
    text: Create a Cloud SQL instance in one zone, and create a failover replica in
      another zone within the same region.
    is_correct: true
- model: sim.answer
  pk: 22
  fields:
    question: 6
    text: Create a Cloud SQL instance in one zone, and create a read replica in another
      zone within the same region.
    is_correct: false
- model: sim.answer
  pk: 23
  fields:
    question: 6
    text: Create a Cloud SQL instance in one zone, and configure an external read
      replica in a zone in a different region.
    is_correct: false
- model: sim.answer
  pk: 24
  fields:
    question: 6
    text: Create a Cloud SQL instance in a region, and configure automatic backup
      to a Cloud Storage bucket in the same region.
    is_correct: false
- model: sim.question
  pk: 7
  fields:
    quiz: 1
    text: You operate an IoT pipeline built around Apache Kafka that normally receives
      around 5000 messages per second. You want to use Google Cloud Platform to create
      an alert as soon as the moving average over 1 hour drops below 4000 messages
      per second. What should you do?
- model: sim.answer
  pk: 25
  fields:
    question: 7
    text: Consume the stream of data in Dataflow using Kafka IO. Set a sliding time
      window of 1 hour every 5 minutes. Compute the average when the window closes,
      and send an alert if the average is less than 4000 messages.
    is_correct: true
- model: sim.answer
  pk: 26
  fields:
    question: 7
    text: Consume the stream of data in Dataflow using Kafka IO. Set a fixed time
      window of 1 hour. Compute the average when the window closes, and send an alert
      if the average is less than 4000 messages.
    is_correct: false
- model: sim.answer
  pk: 27
  fields:
    question: 7
    text: Use Kafka Connect to link your Kafka message queue to Pub/Sub. Use a Dataflow
      template to write your messages from Pub/Sub to Bigtable. Use Cloud Scheduler
      to run a script every hour that counts the number of rows created in Bigtable
      in the last hour. If that number falls below 4000, send an alert.
    is_correct: false
- model: sim.answer
  pk: 28
  fields:
    question: 7
    text: Use Kafka Connect to link your Kafka message queue to Pub/Sub. Use a Dataflow
      template to write your messages from Pub/Sub to BigQuery. Use Cloud Scheduler
      to run a script every five minutes that counts the number of rows created in
      BigQuery in the last hour. If that number falls below 4000, send an alert.
    is_correct: false
- model: sim.question
  pk: 8
  fields:
    quiz: 1
    text: Your company is migrating their 30-node Apache Hadoop cluster to the cloud.
      They want to re-use Hadoop jobs they have already created and minimize the management
      of the cluster as much as possible. They also want to be able to persist data
      beyond the life of the cluster. What should you do?
- model: sim.answer
  pk: 29
  fields:
    question: 8
    text: A. Create a Google Cloud Dataflow job to process the data.
    is_correct: false
- model: sim.answer
  pk: 30
  fields:
    question: 8
    text: B. Create a Google Cloud Dataproc cluster that uses persistent disks for
      HDFS.
    is_correct: false
- model: sim.answer
  pk: 31
  fields:
    question: 8
    text: C. Create a Hadoop cluster on Google Compute Engine that uses persistent
      disks.
    is_correct: false
- model: sim.answer
  pk: 32
  fields:
    question: 8
    text: D. Create a Cloud Dataproc cluster that uses the Google Cloud Storage connector.
    is_correct: true
- model: sim.answer
  pk: 33
  fields:
    question: 8
    text: E. Create a Hadoop cluster on Google Compute Engine that uses Local SSD
      disks.
    is_correct: false
- model: sim.question
  pk: 9
  fields:
    quiz: 1
    text: Your company's on-premises Apache Hadoop servers are approaching end-of-life,
      and IT has decided to migrate the cluster to Google Cloud Dataproc. A like-for-like
      migration of the cluster would require 50 TB of Google Persistent Disk per node.
      The CIO is concerned about the cost of using that much block storage. You want
      to minimize the storage cost of the migration. What should you do?
- model: sim.answer
  pk: 34
  fields:
    question: 9
    text: A. Put the data into Google Cloud Storage.
    is_correct: true
- model: sim.answer
  pk: 35
  fields:
    question: 9
    text: B. Use preemptible virtual machines (VMs) for the Cloud Dataproc cluster.
    is_correct: false
- model: sim.answer
  pk: 36
  fields:
    question: 9
    text: C. Tune the Cloud Dataproc cluster so that there is just enough disk for
      all data.
    is_correct: false
- model: sim.answer
  pk: 37
  fields:
    question: 9
    text: D. Migrate some of the cold data into Google Cloud Storage, and keep only
      the hot data in Persistent Disk.
    is_correct: false
- model: sim.question
  pk: 10
  fields:
    quiz: 1
    text: You designed a database for patient records as a pilot project to cover
      a few hundred patients in three clinics. Your design used a single database
      table to represent all patients and their visits, and you used self-joins to
      generate reports. The server resource utilization was at 50%. Since then, the
      scope of the project has expanded. The database must now store 100 times more
      patient records. You can no longer run the reports, because they either take
      too long or they encounter errors with insufficient compute resources. How should
      you adjust the database design?
- model: sim.answer
  pk: 38
  fields:
    question: 10
    text: Add capacity (memory and disk space) to the database server by the order
      of 200.
    is_correct: false
- model: sim.answer
  pk: 39
  fields:
    question: 10
    text: Shard the tables into smaller ones based on date ranges, and only generate
      reports with prespecified date ranges.
    is_correct: false
- model: sim.answer
  pk: 40
  fields:
    question: 10
    text: Normalize the master patient-record table into the patient table and the
      visits table, and create other necessary tables to avoid self-join.
    is_correct: true
- model: sim.answer
  pk: 41
  fields:
    question: 10
    text: Partition the table into smaller tables, with one for each clinic. Run queries
      against the smaller table pairs, and use unions for consolidated reports.
    is_correct: false
- model: sim.question
  pk: 11
  fields:
    quiz: 1
    text: You create an important report for your large team in Google Data Studio
      360. The report uses Google BigQuery as its data source. You notice that visualizations
      are not showing data that is less than 1 hour old. What should you do?
- model: sim.answer
  pk: 42
  fields:
    question: 11
    text: Disable caching by editing the report settings.
    is_correct: true
- model: sim.answer
  pk: 43
  fields:
    question: 11
    text: Disable caching in BigQuery by editing table details.
    is_correct: false
- model: sim.answer
  pk: 44
  fields:
    question: 11
    text: Refresh your browser tab showing the visualizations.
    is_correct: false
- model: sim.answer
  pk: 45
  fields:
    question: 11
    text: Clear your browser history for the past hour then reload the tab showing
      the visualizations.
    is_correct: false
- model: sim.question
  pk: 12
  fields:
    quiz: 1
    text: An external customer provides you with a daily dump of data from their database.
      The data flows into Google Cloud Storage (GCS) as comma-separated values (CSV)
      files. You want to analyze this data in Google BigQuery, but the data could
      have rows that are formatted incorrectly or corrupted. How should you build
      this pipeline?
- model: sim.answer
  pk: 46
  fields:
    question: 12
    text: Use federated data sources, and check data in the SQL query.
    is_correct: false
- model: sim.answer
  pk: 47
  fields:
    question: 12
    text: Enable BigQuery monitoring in Google Stackdriver and create an alert.
    is_correct: false
- model: sim.answer
  pk: 48
  fields:
    question: 12
    text: Import the data into BigQuery using the gcloud CLI and set max_bad_records
      to 0.
    is_correct: false
- model: sim.answer
  pk: 49
  fields:
    question: 12
    text: Run a Google Cloud Dataflow batch pipeline to import the data into BigQuery,
      and push errors to another dead-letter table for analysis.
    is_correct: true
- model: sim.question
  pk: 13
  fields:
    quiz: 1
    text: Your weather app queries a database every 15 minutes to get the current
      temperature. The frontend is powered by Google App Engine and serves millions
      of users. How should you design the frontend to respond to a database failure?
- model: sim.answer
  pk: 50
  fields:
    question: 13
    text: Issue a command to restart the database servers.
    is_correct: false
- model: sim.answer
  pk: 51
  fields:
    question: 13
    text: Retry the query with exponential backoff, up to a cap of 15 minutes.
    is_correct: true
- model: sim.answer
  pk: 52
  fields:
    question: 13
    text: Retry the query every second until it comes back online to minimize staleness
      of data.
    is_correct: false
- model: sim.answer
  pk: 53
  fields:
    question: 13
    text: Reduce the query frequency to once every hour until the database comes back
      online.
    is_correct: false
- model: sim.question
  pk: 14
  fields:
    quiz: 1
    text: You are building a new real-time data warehouse for your company and will
      use Google BigQuery streaming inserts. There is no guarantee that data will
      only be sent in once, but you do have a unique ID for each row of data and an
      event timestamp. You want to ensure that duplicates are not included while interactively
      querying data. Which query type should you use?
- model: sim.answer
  pk: 54
  fields:
    question: 14
    text: Include ORDER BY DESC on timestamp column and LIMIT to 1.
    is_correct: false
- model: sim.answer
  pk: 55
  fields:
    question: 14
    text: Use GROUP BY on the unique ID column and timestamp column and SUM on the
      values.
    is_correct: false
- model: sim.answer
  pk: 56
  fields:
    question: 14
    text: Use the LAG window function with PARTITION by unique ID along with WHERE
      LAG IS NOT NULL.
    is_correct: false
- model: sim.answer
  pk: 57
  fields:
    question: 14
    text: Use the ROW_NUMBER window function with PARTITION by unique ID along with
      WHERE row equals 1.
    is_correct: true
- model: sim.question
  pk: 15
  fields:
    quiz: 1
    text: 'You are designing a basket abandonment system for an ecommerce company.
      The system will send a message to a user based on these rules:

      1. No interaction by the user on the site for 1 hour

      2. Has added more than $30 worth of products to the basket

      3. Has not completed a transaction

      You use Google Cloud Dataflow to process the data and decide if a message should
      be sent. How should you design the pipeline?

      '
- model: sim.answer
  pk: 58
  fields:
    question: 15
    text: Use a fixed-time window with a duration of 60 minutes.
    is_correct: false
- model: sim.answer
  pk: 59
  fields:
    question: 15
    text: Use a sliding time window with a duration of 60 minutes.
    is_correct: false
- model: sim.answer
  pk: 60
  fields:
    question: 15
    text: Use a session window with a gap time duration of 60 minutes.
    is_correct: true
- model: sim.answer
  pk: 61
  fields:
    question: 15
    text: Use a global window with a time-based trigger with a delay of 60 minutes.
    is_correct: false
- model: sim.question
  pk: 16
  fields:
    quiz: 1
    text: You want to process payment transactions in a point-of-sale application
      that will run on Google Cloud Platform. Your user base could grow exponentially,
      but you do not want to manage infrastructure scaling. Which Google database
      service should you use?
- model: sim.answer
  pk: 62
  fields:
    question: 16
    text: Cloud SQL
    is_correct: false
- model: sim.answer
  pk: 63
  fields:
    question: 16
    text: BigQuery
    is_correct: false
- model: sim.answer
  pk: 64
  fields:
    question: 16
    text: Cloud Bigtable
    is_correct: false
- model: sim.answer
  pk: 65
  fields:
    question: 16
    text: Cloud Datastore
    is_correct: true
- model: sim.question
  pk: 17
  fields:
    quiz: 1
    text: You have historical data covering the last three years in BigQuery and a
      data pipeline that delivers new data to BigQuery daily. You have noticed that
      when the Data Science team runs a query filtered on a date column and limited
      to 30-90 days of data, the query scans the entire table. You also noticed that
      your bill is increasing more quickly than you expected. You want to resolve
      the issue as cost-effectively as possible while maintaining the ability to conduct
      SQL queries. What should you do?
- model: sim.answer
  pk: 66
  fields:
    question: 17
    text: Re-create the tables using DDL. Partition the tables by a column containing
      a TIMESTAMP or DATE Type.
    is_correct: true
- model: sim.answer
  pk: 67
  fields:
    question: 17
    text: Recommend that the Data Science team export the table to a CSV file on Cloud
      Storage and use Cloud Datalab to explore the data by reading the files directly.
    is_correct: false
- model: sim.answer
  pk: 68
  fields:
    question: 17
    text: Modify your pipeline to maintain the last 30-90 days of data in one table
      and the longer history in a different table to minimize full table scans over
      the entire history.
    is_correct: false
- model: sim.answer
  pk: 69
  fields:
    question: 17
    text: Write an Apache Beam pipeline that creates a BigQuery table per day. Recommend
      that the Data Science team use wildcards on the table name suffixes to select
      the data they need.
    is_correct: false
- model: sim.question
  pk: 18
  fields:
    quiz: 1
    text: You operate a logistics company, and you want to improve event delivery
      reliability for vehicle-based sensors. You operate small data centers around
      the world to capture these events, but leased lines that provide connectivity
      from your event collection infrastructure to your event processing infrastructure
      are unreliable, with unpredictable latency. You want to address this issue in
      the most cost-effective way. What should you do?
- model: sim.answer
  pk: 70
  fields:
    question: 18
    text: Deploy small Kafka clusters in your data centers to buffer events.
    is_correct: false
- model: sim.answer
  pk: 71
  fields:
    question: 18
    text: Have the data acquisition devices publish data to Cloud Pub/Sub.
    is_correct: true
- model: sim.answer
  pk: 72
  fields:
    question: 18
    text: Establish a Cloud Interconnect between all remote data centers and Google.
    is_correct: false
- model: sim.answer
  pk: 73
  fields:
    question: 18
    text: Write a Cloud Dataflow pipeline that aggregates all data in session windows.
    is_correct: false
- model: sim.question
  pk: 19
  fields:
    quiz: 1
    text: You are a retailer that wants to integrate your online sales capabilities
      with different in-home assistants, such as Google Home. You need to interpret
      customer voice commands and issue an order to the backend systems. Which solutions
      should you choose?
- model: sim.answer
  pk: 74
  fields:
    question: 19
    text: Speech-to-Text API
    is_correct: false
- model: sim.answer
  pk: 75
  fields:
    question: 19
    text: Cloud Natural Language API
    is_correct: false
- model: sim.answer
  pk: 76
  fields:
    question: 19
    text: Dialogflow Enterprise Edition
    is_correct: true
- model: sim.answer
  pk: 77
  fields:
    question: 19
    text: AutoML Natural Language
    is_correct: false
- model: sim.question
  pk: 20
  fields:
    quiz: 1
    text: Your company has a hybrid cloud initiative. You have a complex data pipeline
      that moves data between cloud provider services and leverages services from
      each of the cloud providers. Which cloud-native service should you use to orchestrate
      the entire pipeline?
- model: sim.answer
  pk: 78
  fields:
    question: 20
    text: Cloud Dataflow
    is_correct: false
- model: sim.answer
  pk: 79
  fields:
    question: 20
    text: Cloud Composer
    is_correct: true
- model: sim.answer
  pk: 80
  fields:
    question: 20
    text: Cloud Dataprep
    is_correct: false
- model: sim.answer
  pk: 81
  fields:
    question: 20
    text: Cloud Dataproc
    is_correct: false
- model: sim.question
  pk: 21
  fields:
    quiz: 1
    text: You use a dataset in BigQuery for analysis. You want to provide third-party
      companies with access to the same dataset. You need to keep the costs of data
      sharing low and ensure that the data is current. Which solution should you choose?
- model: sim.answer
  pk: 82
  fields:
    question: 21
    text: Use Analytics Hub to control data access, and provide third-party companies
      with access to the dataset.
    is_correct: true
- model: sim.answer
  pk: 83
  fields:
    question: 21
    text: Use Cloud Scheduler to export the data on a regular basis to Cloud Storage,
      and provide third-party companies with access to the bucket.
    is_correct: false
- model: sim.answer
  pk: 84
  fields:
    question: 21
    text: Create a separate dataset in BigQuery that contains the relevant data to
      share, and provide third-party companies with access to the new dataset.
    is_correct: false
- model: sim.answer
  pk: 85
  fields:
    question: 21
    text: Create a Dataflow job that reads the data in frequent time intervals, and
      writes it to the relevant BigQuery dataset or Cloud Storage bucket for third-party
      companies to use.
    is_correct: false
- model: sim.question
  pk: 22
  fields:
    quiz: 1
    text: You are designing a data processing pipeline. The pipeline must be able
      to scale automatically as load increases. Messages must be processed at least
      once and must be ordered within windows of 1 hour. How should you design the
      solution?
- model: sim.answer
  pk: 86
  fields:
    question: 22
    text: Use Apache Kafka for message ingestion and use Cloud Dataproc for streaming
      analysis.
    is_correct: false
- model: sim.answer
  pk: 87
  fields:
    question: 22
    text: Use Apache Kafka for message ingestion and use Cloud Dataflow for streaming
      analysis.
    is_correct: false
- model: sim.answer
  pk: 88
  fields:
    question: 22
    text: Use Cloud Pub/Sub for message ingestion and Cloud Dataproc for streaming
      analysis.
    is_correct: false
- model: sim.answer
  pk: 89
  fields:
    question: 22
    text: Use Cloud Pub/Sub for message ingestion and Cloud Dataflow for streaming
      analysis.
    is_correct: true
- model: sim.question
  pk: 23
  fields:
    quiz: 1
    text: You need to set access to BigQuery for different departments within your
      company. Your solution should comply with the following requirements:\n- Each
      department should have access only to their data.\n- Each department will have
      one or more leads who need to be able to create and update tables and provide
      them to their team.\n- Each department has data analysts who need to be able
      to query but not modify data.\nHow should you set access to the data in BigQuery?
- model: sim.answer
  pk: 90
  fields:
    question: 23
    text: Create a dataset for each department. Assign the department leads the role
      of OWNER, and assign the data analysts the role of WRITER on their dataset.
    is_correct: false
- model: sim.answer
  pk: 91
  fields:
    question: 23
    text: Create a dataset for each department. Assign the department leads the role
      of WRITER, and assign the data analysts the role of READER on their dataset.
    is_correct: true
- model: sim.answer
  pk: 92
  fields:
    question: 23
    text: Create a table for each department. Assign the department leads the role
      of Owner, and assign the data analysts the role of Editor on the project the
      table is in.
    is_correct: false
- model: sim.answer
  pk: 93
  fields:
    question: 23
    text: Create a table for each department. Assign the department leads the role
      of Editor, and assign the data analysts the role of Viewer on the project the
      table is in.
    is_correct: false
- model: sim.question
  pk: 24
  fields:
    quiz: 1
    text: You operate a database that stores stock trades and an application that
      retrieves average stock price for a given company over an adjustable window
      of time. The data is stored in Cloud Bigtable where the datetime of the stock
      trade is the beginning of the row key. Your application has thousands of concurrent
      users, and you notice that performance is starting to degrade as more stocks
      are added. What should you do to improve the performance of your application?
- model: sim.answer
  pk: 94
  fields:
    question: 24
    text: Change the row key syntax in your Cloud Bigtable table to begin with the
      stock symbol.
    is_correct: true
- model: sim.answer
  pk: 95
  fields:
    question: 24
    text: Change the row key syntax in your Cloud Bigtable table to begin with a random
      number per second.
    is_correct: false
- model: sim.answer
  pk: 96
  fields:
    question: 24
    text: Change the data pipeline to use BigQuery for storing stock trades, and update
      your application.
    is_correct: false
- model: sim.answer
  pk: 97
  fields:
    question: 24
    text: Use Cloud Dataflow to write a summary of each day's stock trades to an Avro
      file on Cloud Storage. Update your application to read from Cloud Storage and
      Cloud Bigtable to compute the responses.
    is_correct: false
- model: sim.question
  pk: 25
  fields:
    quiz: 1
    text: You are operating a Cloud Dataflow streaming pipeline. The pipeline aggregates
      events from a Cloud Pub/Sub subscription source, within a window, and sinks
      the resulting aggregation to a Cloud Storage bucket. The source has consistent
      throughput. You want to monitor an alert on behavior of the pipeline with Cloud
      Stackdriver to ensure that it is processing data. Which Stackdriver alerts should
      you create?
- model: sim.answer
  pk: 98
  fields:
    question: 25
    text: An alert based on a decrease of subscription/num_undelivered_messages for
      the source and a rate of change increase of instance/storage/used_bytes for
      the destination.
    is_correct: false
- model: sim.answer
  pk: 99
  fields:
    question: 25
    text: An alert based on an increase of subscription/num_undelivered_messages for
      the source and a rate of change decrease of instance/storage/used_bytes for
      the destination.
    is_correct: true
- model: sim.answer
  pk: 100
  fields:
    question: 25
    text: An alert based on a decrease of instance/storage/used_bytes for the source
      and a rate of change increase of subscription/num_undelivered_messages for the
      destination.
    is_correct: false
- model: sim.answer
  pk: 101
  fields:
    question: 25
    text: An alert based on an increase of instance/storage/used_bytes for the source
      and a rate of change decrease of subscription/num_undelivered_messages for the
      destination.
    is_correct: false
- model: sim.question
  pk: 26
  fields:
    quiz: 1
    text: You currently have a single on-premises Kafka cluster in a data center in
      the us-east region that is responsible for ingesting messages from IoT devices
      globally. Because large parts of the globe have poor internet connectivity,
      messages sometimes batch at the edge, come in all at once, and cause a spike
      in load on your Kafka cluster. This is becoming difficult to manage and prohibitively
      expensive. What is the Google-recommended cloud native architecture for this
      scenario?
- model: sim.answer
  pk: 102
  fields:
    question: 26
    text: Edge TPUs as sensor devices for storing and transmitting the messages.
    is_correct: false
- model: sim.answer
  pk: 103
  fields:
    question: 26
    text: Cloud Dataflow connected to the Kafka cluster to scale the processing of
      incoming messages.
    is_correct: false
- model: sim.answer
  pk: 104
  fields:
    question: 26
    text: An IoT gateway connected to Cloud Pub/Sub, with Cloud Dataflow to read and
      process the messages from Cloud Pub/Sub.
    is_correct: true
- model: sim.answer
  pk: 105
  fields:
    question: 26
    text: A Kafka cluster virtualized on Compute Engine in us-east with Cloud Load
      Balancing to connect to the devices around the world.
    is_correct: false
- model: sim.question
  pk: 27
  fields:
    quiz: 1
    text: "You are designing a cloud-native historical data processing system to meet\
      \ the following conditions:\n\xE2\u0153\u2018 The data being analyzed is in\
      \ CSV, Avro, and PDF formats and will be accessed by multiple analysis tools\
      \ including Dataproc, BigQuery, and Compute Engine.\n\xE2\u0153\u2018 A batch\
      \ pipeline moves daily data.\n\xE2\u0153\u2018 Performance is not a factor in\
      \ the solution.\n\xE2\u0153\u2018 The solution design should maximize availability.\n\
      How should you design data storage for this solution?\n"
- model: sim.answer
  pk: 106
  fields:
    question: 27
    text: Create a Dataproc cluster with high availability. Store the data in HDFS,
      and perform analysis as needed.
    is_correct: false
- model: sim.answer
  pk: 107
  fields:
    question: 27
    text: Store the data in BigQuery. Access the data using the BigQuery Connector
      on Dataproc and Compute Engine.
    is_correct: false
- model: sim.answer
  pk: 108
  fields:
    question: 27
    text: Store the data in a regional Cloud Storage bucket. Access the bucket directly
      using Dataproc, BigQuery, and Compute Engine.
    is_correct: false
- model: sim.answer
  pk: 109
  fields:
    question: 27
    text: Store the data in a multi-regional Cloud Storage bucket. Access the data
      directly using Dataproc, BigQuery, and Compute Engine.
    is_correct: true
- model: sim.question
  pk: 28
  fields:
    quiz: 1
    text: You have a petabyte of analytics data and need to design a storage and processing
      platform for it. You must be able to perform data warehouse-style analytics
      on the data in Google Cloud and expose the dataset as files for batch analysis
      tools in other cloud providers. What should you do?
- model: sim.answer
  pk: 110
  fields:
    question: 28
    text: Store and process the entire dataset in BigQuery.
    is_correct: false
- model: sim.answer
  pk: 111
  fields:
    question: 28
    text: Store and process the entire dataset in Bigtable.
    is_correct: false
- model: sim.answer
  pk: 112
  fields:
    question: 28
    text: Store the full dataset in BigQuery, and store a compressed copy of the data
      in a Cloud Storage bucket.
    is_correct: true
- model: sim.answer
  pk: 113
  fields:
    question: 28
    text: Store the warm data as files in Cloud Storage, and store the active data
      in BigQuery. Keep this ratio as 80% warm and 20% active.
    is_correct: false
- model: sim.question
  pk: 29
  fields:
    quiz: 1
    text: You are working on a niche product in the image recognition domain. Your
      team has developed a model that is dominated by custom C++ TensorFlow ops your
      team has implemented. These ops are used inside your main training loop and
      are performing bulky matrix multiplications. It currently takes up to several
      days to train a model. You want to decrease this time significantly and keep
      the cost low by using an accelerator on Google Cloud. What should you do?
- model: sim.answer
  pk: 114
  fields:
    question: 29
    text: Use Cloud TPUs without any additional adjustment to your code.
    is_correct: false
- model: sim.answer
  pk: 115
  fields:
    question: 29
    text: Use Cloud TPUs after implementing GPU kernel support for your customs ops.
    is_correct: false
- model: sim.answer
  pk: 116
  fields:
    question: 29
    text: Use Cloud GPUs after implementing GPU kernel support for your customs ops.
    is_correct: true
- model: sim.answer
  pk: 117
  fields:
    question: 29
    text: Stay on CPUs, and increase the size of the cluster you're training your
      model on.
    is_correct: false
- model: sim.question
  pk: 30
  fields:
    quiz: 1
    text: You use BigQuery as your centralized analytics platform. New data is loaded
      every day, and an ETL pipeline modifies the original data and prepares it for
      the final users. This ETL pipeline is regularly modified and can generate errors,
      but sometimes the errors are detected only after 2 weeks. You need to provide
      a method to recover from these errors, and your backups should be optimized
      for storage costs. How should you organize your data in BigQuery and store your
      backups?
- model: sim.answer
  pk: 118
  fields:
    question: 30
    text: Organize your data in a single table, export, and compress and store the
      BigQuery data in Cloud Storage.
    is_correct: false
- model: sim.answer
  pk: 119
  fields:
    question: 30
    text: Organize your data in separate tables for each month, and export, compress,
      and store the data in Cloud Storage.
    is_correct: true
- model: sim.answer
  pk: 120
  fields:
    question: 30
    text: Organize your data in separate tables for each month, and duplicate your
      data on a separate dataset in BigQuery.
    is_correct: false
- model: sim.answer
  pk: 121
  fields:
    question: 30
    text: Organize your data in separate tables for each month, and use snapshot decorators
      to restore the table to a time prior to the corruption.
    is_correct: false
- model: sim.question
  pk: 31
  fields:
    quiz: 1
    text: The marketing team at your organization provides regular updates of a segment
      of your customer dataset. The marketing team has given you a CSV with 1 million
      records that must be updated in BigQuery. When you use the UPDATE statement
      in BigQuery, you receive a quotaExceeded error. What should you do?
- model: sim.answer
  pk: 122
  fields:
    question: 31
    text: Reduce the number of records updated each day to stay within the BigQuery
      UPDATE DML statement limit.
    is_correct: false
- model: sim.answer
  pk: 123
  fields:
    question: 31
    text: Increase the BigQuery UPDATE DML statement limit in the Quota management
      section of the Google Cloud Platform Console.
    is_correct: false
- model: sim.answer
  pk: 124
  fields:
    question: 31
    text: Split the source CSV file into smaller CSV files in Cloud Storage to reduce
      the number of BigQuery UPDATE DML statements per BigQuery job.
    is_correct: false
- model: sim.answer
  pk: 125
  fields:
    question: 31
    text: Import the new records from the CSV file into a new BigQuery table. Create
      a BigQuery job that merges the new records with the existing records and writes
      the results to a new BigQuery table.
    is_correct: true
- model: sim.question
  pk: 32
  fields:
    quiz: 1
    text: "Your United States-based company has created an application for assessing\
      \ and responding to user actions. The primary table's data volume grows by 250,000\
      \ records per second. Many third parties use your application's APIs to build\
      \ the functionality into their own frontend applications. Your application's\
      \ APIs should comply with the following requirements:\n\xE2\u0153\u2018 Single\
      \ global endpoint\n\xE2\u0153\u2018 ANSI SQL support\n\xE2\u0153\u2018 Consistent\
      \ access to the most up-to-date data\nWhat should you do?\n"
- model: sim.answer
  pk: 126
  fields:
    question: 32
    text: Implement BigQuery with no region selected for storage or processing.
    is_correct: false
- model: sim.answer
  pk: 127
  fields:
    question: 32
    text: Implement Cloud Spanner with the leader in North America and read-only replicas
      in Asia and Europe.
    is_correct: true
- model: sim.answer
  pk: 128
  fields:
    question: 32
    text: Implement Cloud SQL for PostgreSQL with the master in North America and
      read replicas in Asia and Europe.
    is_correct: false
- model: sim.answer
  pk: 129
  fields:
    question: 32
    text: Implement Bigtable with the primary cluster in North America and secondary
      clusters in Asia and Europe.
    is_correct: false
- model: sim.question
  pk: 33
  fields:
    quiz: 1
    text: A data scientist has created a BigQuery ML model and asks you to create
      an ML pipeline to serve predictions. You have a REST API application with the
      requirement to serve predictions for an individual user ID with latency under
      100 milliseconds. You use the following query to generate predictions- SELECT
      predicted_label, user_id FROM ML.PREDICT(MODEL 'dataset.model', table user_features).
      How should you create the ML pipeline?
- model: sim.answer
  pk: 130
  fields:
    question: 33
    text: Add a WHERE clause to the query, and grant the BigQuery Data Viewer role
      to the application service account.
    is_correct: false
- model: sim.answer
  pk: 131
  fields:
    question: 33
    text: Create an Authorized View with the provided query. Share the dataset that
      contains the view with the application service account.
    is_correct: false
- model: sim.answer
  pk: 132
  fields:
    question: 33
    text: Create a Dataflow pipeline using BigQueryIO to read results from the query.
      Grant the Dataflow Worker role to the application service account.
    is_correct: false
- model: sim.answer
  pk: 133
  fields:
    question: 33
    text: Create a Dataflow pipeline using BigQueryIO to read predictions for all
      users from the query. Write the results to Bigtable using BigtableIO. Grant
      the Bigtable Reader role to the application service account so that the application
      can read predictions for individual users from Bigtable.
    is_correct: true
- model: sim.question
  pk: 34
  fields:
    quiz: 1
    text: |
      You are building an application to share financial market data with consumers, who will receive data feeds. Data is
      collected from the markets in real time.
      Consumers will receive the data in the following ways:
      - Real-time event stream
      - ANSI SQL access to real-time stream and historical data
      - Batch historical exports
      Which solution should you use?

- model: sim.answer
  pk: 134
  fields:
    question: 34
    text: Cloud Dataflow, Cloud SQL, Cloud Spanner
    is_correct: false
- model: sim.answer
  pk: 135
  fields:
    question: 34
    text: Cloud Pub/Sub, Cloud Storage, BigQuery
    is_correct: true
- model: sim.answer
  pk: 136
  fields:
    question: 34
    text: Cloud Dataproc, Cloud Dataflow, BigQuery
    is_correct: false
- model: sim.answer
  pk: 137
  fields:
    question: 34
    text: Cloud Pub/Sub, Cloud Dataproc, Cloud SQL
    is_correct: false
- model: sim.question
  pk: 35
  fields:
    quiz: 1
    text: "You are building a new application that you need to collect data from in\
      \ a scalable way. Data arrives continuously from the application throughout\
      \ the day, and you expect to generate approximately 150 GB of JSON data per\
      \ day by the end of the year. Your requirements are:\n\xE2\u0153\u2018 Decoupling\
      \ producer from consumer\n\xE2\u0153\u2018 Space and cost-efficient storage\
      \ of the raw ingested data, which is to be stored indefinitely\n\xE2\u0153\u2018\
      \ Near real-time SQL query\n\xE2\u0153\u2018 Maintain at least 2 years of historical\
      \ data, which will be queried with SQL\nWhich pipeline should you use to meet\
      \ these requirements?\n"
- model: sim.answer
  pk: 138
  fields:
    question: 35
    text: Create an application that provides an API. Write a tool to poll the API
      and write data to Cloud Storage as gzipped JSON files.
    is_correct: false
- model: sim.answer
  pk: 139
  fields:
    question: 35
    text: Create an application that writes to a Cloud SQL database to store the data.
      Set up periodic exports of the database to write to Cloud Storage and load into
      BigQuery.
    is_correct: false
- model: sim.answer
  pk: 140
  fields:
    question: 35
    text: Create an application that publishes events to Cloud Pub/Sub, and create
      Spark jobs on Cloud Dataproc to convert the JSON data to Avro format, stored
      on HDFS on Persistent Disk.
    is_correct: false
- model: sim.answer
  pk: 141
  fields:
    question: 35
    text: Create an application that publishes events to Cloud Pub/Sub, and create
      a Cloud Dataflow pipeline that transforms the JSON event payloads to Avro, writing
      the data to Cloud Storage and BigQuery.
    is_correct: true
- model: sim.question
  pk: 36
  fields:
    quiz: 1
    text: You have several Spark jobs that run on a Cloud Dataproc cluster on a schedule.
      Some of the jobs run in sequence, and some of the jobs run concurrently. You
      need to automate this process. What should you do?
- model: sim.answer
  pk: 142
  fields:
    question: 36
    text: Create a Cloud Dataproc Workflow Template
    is_correct: true
- model: sim.answer
  pk: 143
  fields:
    question: 36
    text: Create an initialization action to execute the jobs
    is_correct: false
- model: sim.answer
  pk: 144
  fields:
    question: 36
    text: Create a Directed Acyclic Graph in Cloud Composer
    is_correct: false
- model: sim.answer
  pk: 145
  fields:
    question: 36
    text: Create a Bash script that uses the Cloud SDK to create a cluster, execute
      jobs, and then tear down the cluster
    is_correct: false
- model: sim.question
  pk: 37
  fields:
    quiz: 1
    text: You are building a new data pipeline to share data between two different
      types of applications- jobs generators and job runners. Your solution must scale
      to accommodate increases in usage and must accommodate the addition of new applications
      without negatively affecting the performance of existing ones. What should you
      do?
- model: sim.answer
  pk: 146
  fields:
    question: 37
    text: Create an API using App Engine to receive and send messages to the applications
    is_correct: false
- model: sim.answer
  pk: 147
  fields:
    question: 37
    text: Use a Cloud Pub/Sub topic to publish jobs, and use subscriptions to execute
      them
    is_correct: true
- model: sim.answer
  pk: 148
  fields:
    question: 37
    text: Create a table on Cloud SQL, and insert and delete rows with the job information
    is_correct: false
- model: sim.answer
  pk: 149
  fields:
    question: 37
    text: Create a table on Cloud Spanner, and insert and delete rows with the job
      information
    is_correct: false
- model: sim.question
  pk: 38
  fields:
    quiz: 1
    text: You need to create a new transaction table in Cloud Spanner that stores
      product sales data. You are deciding what to use as a primary key. From a performance
      perspective, which strategy should you choose?
- model: sim.answer
  pk: 150
  fields:
    question: 38
    text: The current epoch time
    is_correct: false
- model: sim.answer
  pk: 151
  fields:
    question: 38
    text: A concatenation of the product name and the current epoch time
    is_correct: false
- model: sim.answer
  pk: 152
  fields:
    question: 38
    text: A random universally unique identifier number (version 4 UUID)
    is_correct: true
- model: sim.answer
  pk: 153
  fields:
    question: 38
    text: The original order identification number from the sales system, which is
      a monotonically increasing integer
    is_correct: false
- model: sim.question
  pk: 39
  fields:
    quiz: 1
    text: Data Analysts in your company have the Cloud IAM Owner role assigned to
      them in their projects to allow them to work with multiple GCP products in their
      projects. Your organization requires that all BigQuery data access logs be retained
      for 6 months. You need to ensure that only audit personnel in your company can
      access the data access logs for all projects. What should you do?
- model: sim.answer
  pk: 154
  fields:
    question: 39
    text: Enable data access logs in each Data Analyst's project. Restrict access
      to Stackdriver Logging via Cloud IAM roles.
    is_correct: false
- model: sim.answer
  pk: 155
  fields:
    question: 39
    text: Export the data access logs via a project-level export sink to a Cloud Storage
      bucket in the Data Analysts' projects. Restrict access to the Cloud Storage
      bucket.
    is_correct: false
- model: sim.answer
  pk: 156
  fields:
    question: 39
    text: Export the data access logs via a project-level export sink to a Cloud Storage
      bucket in a newly created project for audit logs. Restrict access to the project
      with the exported logs.
    is_correct: false
- model: sim.answer
  pk: 157
  fields:
    question: 39
    text: Export the data access logs via an aggregated export sink to a Cloud Storage
      bucket in a newly created project for audit logs. Restrict access to the project
      that contains the exported logs.
    is_correct: true
- model: sim.question
  pk: 40
  fields:
    quiz: 1
    text: Each analytics team in your organization is running BigQuery jobs in their
      own projects. You want to enable each team to monitor slot usage within their
      projects. What should you do?
- model: sim.answer
  pk: 158
  fields:
    question: 40
    text: Create a Cloud Monitoring dashboard based on the BigQuery metric query/scanned_bytes
    is_correct: false
- model: sim.answer
  pk: 159
  fields:
    question: 40
    text: Create a Cloud Monitoring dashboard based on the BigQuery metric slots/allocated_for_project
    is_correct: true
- model: sim.answer
  pk: 160
  fields:
    question: 40
    text: Create a log export for each project, capture the BigQuery job execution
      logs, create a custom metric based on the totalSlotMs, and create a Cloud Monitoring
      dashboard based on the custom metric
    is_correct: false
- model: sim.answer
  pk: 161
  fields:
    question: 40
    text: Create an aggregated log export at the organization level, capture the BigQuery
      job execution logs, create a custom metric based on the totalSlotMs, and create
      a Cloud Monitoring dashboard based on the custom metric
    is_correct: false
- model: sim.question
  pk: 41
  fields:
    quiz: 1
    text: You are operating a streaming Cloud Dataflow pipeline. Your engineers have
      a new version of the pipeline with a different windowing algorithm and triggering
      strategy. You want to update the running pipeline with the new version. You
      want to ensure that no data is lost during the update. What should you do?
- model: sim.answer
  pk: 162
  fields:
    question: 41
    text: Update the Cloud Dataflow pipeline inflight by passing the --update option
      with the --jobName set to the existing job name
    is_correct: false
- model: sim.answer
  pk: 163
  fields:
    question: 41
    text: Update the Cloud Dataflow pipeline inflight by passing the --update option
      with the --jobName set to a new unique job name
    is_correct: false
- model: sim.answer
  pk: 164
  fields:
    question: 41
    text: Stop the Cloud Dataflow pipeline with the Cancel option. Create a new Cloud
      Dataflow job with the updated code
    is_correct: false
- model: sim.answer
  pk: 165
  fields:
    question: 41
    text: Stop the Cloud Dataflow pipeline with the Drain option. Create a new Cloud
      Dataflow job with the updated code
    is_correct: true
- model: sim.question
  pk: 42
  fields:
    quiz: 1
    text: You need to move 2 PB of historical data from an on-premises storage appliance
      to Cloud Storage within six months, and your outbound network capacity is constrained
      to 20 Mb/sec. How should you migrate this data to Cloud Storage?
- model: sim.answer
  pk: 166
  fields:
    question: 42
    text: Use Transfer Appliance to copy the data to Cloud Storage
    is_correct: true
- model: sim.answer
  pk: 167
  fields:
    question: 42
    text: Use gsutil cp "J to compress the content being uploaded to Cloud Storage
    is_correct: false
- model: sim.answer
  pk: 168
  fields:
    question: 42
    text: Create a private URL for the historical data, and then use Storage Transfer
      Service to copy the data to Cloud Storage
    is_correct: false
- model: sim.answer
  pk: 169
  fields:
    question: 42
    text: Use trickle or ionice along with gsutil cp to limit the amount of bandwidth
      gsutil utilizes to less than 20 Mb/sec so it does not interfere with the production
      traffic
    is_correct: false
- model: sim.question
  pk: 43
  fields:
    quiz: 1
    text: "You receive data files in CSV format monthly from a third party. You need\
      \ to cleanse this data, but every third month the schema of the files changes.\
      \ Your requirements for implementing these transformations include:\n\xE2\u0153\
      \u2018 Executing the transformations on a schedule\n\xE2\u0153\u2018 Enabling\
      \ non-developer analysts to modify transformations\n\xE2\u0153\u2018 Providing\
      \ a graphical tool for designing transformations\nWhat should you do?\n"
- model: sim.answer
  pk: 170
  fields:
    question: 43
    text: Use Dataprep by Trifacta to build and maintain the transformation recipes,
      and execute them on a scheduled basis
    is_correct: true
- model: sim.answer
  pk: 171
  fields:
    question: 43
    text: Load each month's CSV data into BigQuery, and write a SQL query to transform
      the data to a standard schema. Merge the transformed tables together with a
      SQL query
    is_correct: false
- model: sim.answer
  pk: 172
  fields:
    question: 43
    text: Help the analysts write a Dataflow pipeline in Python to perform the transformation.
      The Python code should be stored in a revision control system and modified as
      the incoming data's schema changes
    is_correct: false
- model: sim.answer
  pk: 173
  fields:
    question: 43
    text: Use Apache Spark on Dataproc to infer the schema of the CSV file before
      creating a Dataframe. Then implement the transformations in Spark SQL before
      writing the data out to Cloud Storage and loading into BigQuery
    is_correct: false
- model: sim.question
  pk: 44
  fields:
    quiz: 1
    text: You are implementing several batch jobs that must be executed on a schedule.
      These jobs have many interdependent steps that must be executed in a specific
      order. Portions of the jobs involve executing shell scripts, running Hadoop
      jobs, and running queries in BigQuery. The jobs are expected to run for many
      minutes up to several hours. If the steps fail, they must be retried a fixed
      number of times. Which service should you use to manage the execution of these
      jobs?
- model: sim.answer
  pk: 174
  fields:
    question: 44
    text: Cloud Scheduler
    is_correct: false
- model: sim.answer
  pk: 175
  fields:
    question: 44
    text: Cloud Dataflow
    is_correct: false
- model: sim.answer
  pk: 176
  fields:
    question: 44
    text: Cloud Functions
    is_correct: false
- model: sim.answer
  pk: 177
  fields:
    question: 44
    text: Cloud Composer
    is_correct: true
- model: sim.question
  pk: 45
  fields:
    quiz: 1
    text: You are migrating your data warehouse to BigQuery. You have migrated all
      of your data into tables in a dataset. Multiple users from your organization
      will be using the data. They should only see certain tables based on their team
      membership. How should you set user permissions?
- model: sim.answer
  pk: 178
  fields:
    question: 45
    text: Assign the users/groups data viewer access at the table level for each table
    is_correct: true
- model: sim.answer
  pk: 179
  fields:
    question: 45
    text: Create SQL views for each team in the same dataset in which the data resides,
      and assign the users/groups data viewer access to the SQL views
    is_correct: false
- model: sim.answer
  pk: 180
  fields:
    question: 45
    text: Create authorized views for each team in the same dataset in which the data
      resides, and assign the users/groups data viewer access to the authorized views
    is_correct: false
- model: sim.answer
  pk: 181
  fields:
    question: 45
    text: Create authorized views for each team in datasets created for each team.
      Assign the authorized views data viewer access to the dataset in which the data
      resides. Assign the users/groups data viewer access to the datasets in which
      the authorized views reside
    is_correct: false
- model: sim.question
  pk: 46
  fields:
    quiz: 1
    text: You want to build a managed Hadoop system as your data lake. The data transformation
      process is composed of a series of Hadoop jobs executed in sequence. To accomplish
      the design of separating storage from compute, you decided to use the Cloud
      Storage connector to store all input data, output data, and intermediary data.
      However, you noticed that one Hadoop job runs very slowly with Cloud Dataproc,
      when compared with the on-premises bare-metal Hadoop environment (8-core nodes
      with 100-GB RAM). Analysis shows that this particular Hadoop job is disk I/O
      intensive. You want to resolve the issue. What should you do?
- model: sim.answer
  pk: 182
  fields:
    question: 46
    text: Allocate sufficient memory to the Hadoop cluster, so that the intermediary
      data of that particular Hadoop job can be held in memory
    is_correct: false
- model: sim.answer
  pk: 183
  fields:
    question: 46
    text: Allocate sufficient persistent disk space to the Hadoop cluster, and store
      the intermediate data of that particular Hadoop job on native HDFS
    is_correct: true
- model: sim.answer
  pk: 184
  fields:
    question: 46
    text: Allocate more CPU cores of the virtual machine instances of the Hadoop cluster
      so that the networking bandwidth for each instance can scale up
    is_correct: false
- model: sim.answer
  pk: 185
  fields:
    question: 46
    text: Allocate additional network interface card (NIC), and configure link aggregation
      in the operating system to use the combined throughput when working with Cloud
      Storage
    is_correct: false
- model: sim.question
  pk: 47
  fields:
    quiz: 1
    text: You work for an advertising company, and you've developed a Spark ML model
      to predict click-through rates at advertisement blocks. You've been developing
      everything at your on-premises data center, and now your company is migrating
      to Google Cloud. Your data center will be closing soon, so a rapid lift-and-shift
      migration is necessary. However, the data you've been using will be migrated
      to BigQuery. You periodically retrain your Spark ML models, so you need to migrate
      existing training pipelines to Google Cloud. What should you do?
- model: sim.answer
  pk: 186
  fields:
    question: 47
    text: Use Vertex AI for training existing Spark ML models
    is_correct: false
- model: sim.answer
  pk: 187
  fields:
    question: 47
    text: Rewrite your models on TensorFlow, and start using Vertex AI
    is_correct: false
- model: sim.answer
  pk: 188
  fields:
    question: 47
    text: Use Dataproc for training existing Spark ML models, but start reading data
      directly from BigQuery
    is_correct: true
- model: sim.answer
  pk: 189
  fields:
    question: 47
    text: Spin up a Spark cluster on Compute Engine, and train Spark ML models on
      the data exported from BigQuery
    is_correct: false
- model: sim.question
  pk: 48
  fields:
    quiz: 1
    text: You work for a global shipping company. You want to train a model on 40
      TB of data to predict which ships in each geographic region are likely to cause
      delivery delays on any given day. The model will be based on multiple attributes
      collected from multiple sources. Telemetry data, including location in GeoJSON
      format, will be pulled from each ship and loaded every hour. You want to have
      a dashboard that shows how many and which ships are likely to cause delays within
      a region. You want to use a storage solution that has native functionality for
      prediction and geospatial processing. Which storage solution should you use?
- model: sim.answer
  pk: 190
  fields:
    question: 48
    text: BigQuery
    is_correct: true
- model: sim.answer
  pk: 191
  fields:
    question: 48
    text: Cloud Bigtable
    is_correct: false
- model: sim.answer
  pk: 192
  fields:
    question: 48
    text: Cloud Datastore
    is_correct: false
- model: sim.answer
  pk: 193
  fields:
    question: 48
    text: Cloud SQL for PostgreSQL
    is_correct: false
- model: sim.question
  pk: 49
  fields:
    quiz: 1
    text: You need to store and analyze social media postings in Google BigQuery at
      a rate of 10,000 messages per minute in near real-time. Initially, design the
      application to use streaming inserts for individual postings. Your application
      also performs data aggregations right after the streaming inserts. You discover
      that the queries after streaming inserts do not exhibit strong consistency,
      and reports from the queries might miss in-flight data. How can you adjust your
      application design?
- model: sim.answer
  pk: 194
  fields:
    question: 49
    text: Re-write the application to load accumulated data every 2 minutes.
    is_correct: false
- model: sim.answer
  pk: 195
  fields:
    question: 49
    text: Convert the streaming insert code to batch load for individual messages.
    is_correct: false
- model: sim.answer
  pk: 196
  fields:
    question: 49
    text: Load the original message to Google Cloud SQL, and export the table every
      hour to BigQuery via streaming inserts.
    is_correct: false
- model: sim.answer
  pk: 197
  fields:
    question: 49
    text: Estimate the average latency for data availability after streaming inserts,
      and always run queries after waiting twice as long.
    is_correct: true
- model: sim.question
  pk: 50
  fields:
    quiz: 1
    text: Your startup has never implemented a formal security policy. Currently,
      everyone in the company has access to the datasets stored in Google BigQuery.
      Teams have freedom to use the service as they see fit, and they have not documented
      their use cases. You have been asked to secure the data warehouse. You need
      to discover what everyone is doing. What should you do first?
- model: sim.answer
  pk: 198
  fields:
    question: 50
    text: Use Google Stackdriver Audit Logs to review data access.
    is_correct: true
- model: sim.answer
  pk: 199
  fields:
    question: 50
    text: Get the identity and access management (IAM) policy of each table
    is_correct: false
- model: sim.answer
  pk: 200
  fields:
    question: 50
    text: Use Stackdriver Monitoring to see the usage of BigQuery query slots.
    is_correct: false
- model: sim.answer
  pk: 201
  fields:
    question: 50
    text: Use the Google Cloud Billing API to see what account the warehouse is being
      billed to.
    is_correct: false
- model: sim.question
  pk: 51
  fields:
    quiz: 1
    text: Your company is migrating their 30-node Apache Hadoop cluster to the cloud.
      They want to re-use Hadoop jobs they have already created and minimize the management
      of the cluster as much as possible. They also want to be able to persist data
      beyond the life of the cluster. What should you do?
- model: sim.answer
  pk: 202
  fields:
    question: 51
    text: Create a Google Cloud Dataflow job to process the data.
    is_correct: false
- model: sim.answer
  pk: 203
  fields:
    question: 51
    text: Create a Google Cloud Dataproc cluster that uses persistent disks for HDFS.
    is_correct: false
- model: sim.answer
  pk: 204
  fields:
    question: 51
    text: Create a Hadoop cluster on Google Compute Engine that uses persistent disks.
    is_correct: false
- model: sim.answer
  pk: 205
  fields:
    question: 51
    text: Create a Cloud Dataproc cluster that uses the Google Cloud Storage connector.
    is_correct: true
- model: sim.answer
  pk: 206
  fields:
    question: 51
    text: Create a Hadoop cluster on Google Compute Engine that uses Local SSD disks.
    is_correct: false
